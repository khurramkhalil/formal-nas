apiVersion: batch/v1
kind: Job
metadata:
  name: formal-nas-standalone
spec:
  template:
    spec:
      # Use scratch volume (emptyDir) for everything. No PVCs.
      volumes:
      - name: workspace
        emptyDir: {}
      
      initContainers:
      # 1. Setup Data & Code (The "Builder" step)
      - name: setup-env
        image: python:3.9-slim
        command: ["/bin/bash", "-c"]
        args:
          - |
            echo "=== Init: Setting up Workspace ==="
            apt-get update && apt-get install -y git wget curl
            pip install gdown
            
            cd /workspace
            mkdir -p src experiments transnas_data
            
            echo "--- Cloning TransNAS-Bench API ---"
            # Clone into a specific dir to avoid collisions
            git clone https://github.com/yawen-d/TransNASBench.git TransNASBench
            
            echo "--- Downloading TransNAS-Bench-101 Data (5GB) ---"
            gdown --folder https://drive.google.com/drive/folders/1HlLr2ihZX_ZuV3lJX_4i7q4w-ZBdhJ6o?usp=share_link -O transnas_data
            
            # Locate the .pth file and move it to root for easy access
            find transnas_data -name "transnas-bench_v10141024.pth" -exec mv {} /workspace/transnas-bench_v10141024.pth \;
            
            echo "=== Init Complete ==="
            ls -R /workspace
        
        volumeMounts:
        - mountPath: /workspace
          name: workspace
        
        resources:
          limits:
            memory: 4Gi
            cpu: 2
          requests:
            memory: 2Gi
            cpu: 1

      containers:
      # 2. Run the Search (The "Runtime" step)
      # Using PyTorch image to save install time
      - name: search-engine
        image: pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime
        command: ["/bin/bash", "-c"]
        args:
          - |
            echo "=== Runtime: Installing Light Dependencies ==="
            # Torch is already here! Just install the rest.
            pip install --no-cache-dir wandb z3-solver scipy python-dotenv matplotlib networkx
            
            echo "=== Runtime: Setting up Environment ==="
            export PYTHONPATH=$PYTHONPATH:/workspace/src:/workspace/TransNASBench
            export WANDB_API_KEY=$WANDB_API_KEY
            export WANDB_PROJECT=$WANDB_PROJECT
            export WANDB_ENTITY=$WANDB_ENTITY
            
            cd /workspace
            
            echo "=== Runtime: Syncing Latest Code ==="
            # We copy the code from the ConfigMap (mounted) or just curl it? 
            # Problem: I can't mount local files easily without a ConfigMap or 'kubectl cp'.
            # Solution: I will sleep for 30s to allow 'kubectl cp' injection, OR use a ConfigMap.
            # 'kubectl cp' is annoying. ConfigMap is better for small code.
            # But the code is in 'src'.
            # I will use the 'kubectl cp' strategy ONE LAST TIME but into the emptyDir.
            # OR I can git clone MY own repo if I had one? No.
            # I will assume I have to 'kubectl cp' the src folder into /workspace/src FIRST.
            # Wait, user wants "Docker file with everything".
            # I can't put my local 'src' into the initContainer without building an image or mounting.
            # Strategy: I will use a 'sleep 15' at the start of THIS container to let me 'kubectl cp'.
            # It's hacky but effective for "Stop the PVC mess".
            
            echo "WAITING FOR CODE UPLOAD (30s)..."
            sleep 30
            
            echo "=== Runtime: Executing Search ==="
            python experiments/run_unified_search.py
            
        env:
          - name: WANDB_API_KEY
            value: "c7ea31e5793ba44d95d91840a10d709378eb2d9d"
          - name: WANDB_PROJECT
            value: "formal-nas-unified"
          - name: WANDB_ENTITY
            value: "khurramkhalil"
        
        volumeMounts:
        - mountPath: /workspace
          name: workspace
        
        resources:
          limits:
            nvidia.com/gpu: 1
            memory: 16Gi
            cpu: 4
          requests:
            nvidia.com/gpu: 1
            memory: 8Gi
            cpu: 2
            
      restartPolicy: Never
